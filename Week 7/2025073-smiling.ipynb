{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":37705,"sourceType":"datasetVersion","datasetId":29561,"isSourceIdPinned":false}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport pandas as pd \nfrom torch.utils.data import Dataset, DataLoader\nimport os \nfrom PIL import Image\nfrom torch.utils.data import random_split\nimport torch \nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchinfo import summary\nfrom torch.utils.data import random_split\nimport torch ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T06:38:13.522488Z","iopub.execute_input":"2025-07-03T06:38:13.522758Z","iopub.status.idle":"2025-07-03T06:38:16.761731Z","shell.execute_reply.started":"2025-07-03T06:38:13.522735Z","shell.execute_reply":"2025-07-03T06:38:16.760880Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"trans = transforms.Compose(\n    [\n        transforms.Resize((64,64)),\n        transforms.ToTensor()\n    ]\n)\n\ntarget_folder = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/\"\nfile = \"/kaggle/input/celeba-dataset/list_attr_celeba.csv\"\nclass FaceImage(Dataset):\n    def __init__(self, label_file, img_dir, transform=None):\n        df = pd.read_csv(label_file, delimiter=\",\")\n        self.labels  = df[['image_id', 'Smiling']].copy()\n        self.labels.replace({-1 : 0},  inplace=True)\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.labels.iloc[idx,0] ) \n        img = Image.open(img_path).convert(\"RGB\")\n        label = self.labels.iloc[idx,1]\n\n        if self.transform:\n            img = self.transform(img)\n            \n        return img, label \n\n    def __len__(self):\n        return self.labels.shape[0]\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T06:38:16.763145Z","iopub.execute_input":"2025-07-03T06:38:16.763527Z","iopub.status.idle":"2025-07-03T06:38:16.769704Z","shell.execute_reply.started":"2025-07-03T06:38:16.763507Z","shell.execute_reply":"2025-07-03T06:38:16.769107Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"data = FaceImage(file,target_folder , trans)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T06:38:16.770420Z","iopub.execute_input":"2025-07-03T06:38:16.770663Z","iopub.status.idle":"2025-07-03T06:38:17.423220Z","shell.execute_reply.started":"2025-07-03T06:38:16.770645Z","shell.execute_reply":"2025-07-03T06:38:17.422391Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"gen = torch.Generator().manual_seed(42)\ntrain_dataset, val_dataset, test_dataset = random_split(data, [141819, 20261, 40519],\n                                        generator=gen)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T06:38:17.424797Z","iopub.execute_input":"2025-07-03T06:38:17.425331Z","iopub.status.idle":"2025-07-03T06:38:17.442528Z","shell.execute_reply.started":"2025-07-03T06:38:17.425308Z","shell.execute_reply":"2025-07-03T06:38:17.441945Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T06:38:17.443240Z","iopub.execute_input":"2025-07-03T06:38:17.443516Z","iopub.status.idle":"2025-07-03T06:38:17.470103Z","shell.execute_reply.started":"2025-07-03T06:38:17.443492Z","shell.execute_reply":"2025-07-03T06:38:17.469457Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"if torch.cuda.is_available():\n    DEVICE = torch.device('cuda')\nelse:\n    DEVICE = torch.device('cpu')\nprint('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T06:39:13.676718Z","iopub.execute_input":"2025-07-03T06:39:13.677014Z","iopub.status.idle":"2025-07-03T06:39:13.776894Z","shell.execute_reply.started":"2025-07-03T06:39:13.676990Z","shell.execute_reply":"2025-07-03T06:39:13.776184Z"}},"outputs":[{"name":"stdout","text":"Using PyTorch version: 2.6.0+cu124  Device: cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 3, padding = 1)\n        self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 3, padding = 1)\n        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n        self.fc1 = nn.Linear(16 * 16 * 16, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, 1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.pool(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = self.pool(x)\n\n        x = x.view(-1, 16 * 16 * 16)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        x = F.relu(x)\n        x = self.fc3(x)\n        x = torch.sigmoid(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T06:39:45.785646Z","iopub.execute_input":"2025-07-03T06:39:45.786484Z","iopub.status.idle":"2025-07-03T06:39:45.793149Z","shell.execute_reply.started":"2025-07-03T06:39:45.786452Z","shell.execute_reply":"2025-07-03T06:39:45.792301Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model = CNN().to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\ncriterion = nn.BCELoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T06:40:28.181566Z","iopub.execute_input":"2025-07-03T06:40:28.182325Z","iopub.status.idle":"2025-07-03T06:40:28.417459Z","shell.execute_reply.started":"2025-07-03T06:40:28.182289Z","shell.execute_reply":"2025-07-03T06:40:28.416858Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"BATCH_SIZE = 128\nEPOCHS = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T06:42:02.109880Z","iopub.execute_input":"2025-07-03T06:42:02.110551Z","iopub.status.idle":"2025-07-03T06:42:02.114146Z","shell.execute_reply.started":"2025-07-03T06:42:02.110522Z","shell.execute_reply":"2025-07-03T06:42:02.113374Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"summary(model, input_size=(32,3,64,64))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T06:42:19.323544Z","iopub.execute_input":"2025-07-03T06:42:19.323829Z","iopub.status.idle":"2025-07-03T06:42:20.933911Z","shell.execute_reply.started":"2025-07-03T06:42:19.323806Z","shell.execute_reply":"2025-07-03T06:42:20.933260Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCNN                                      [32, 1]                   --\n├─Conv2d: 1-1                            [32, 8, 64, 64]           224\n├─MaxPool2d: 1-2                         [32, 8, 32, 32]           --\n├─Conv2d: 1-3                            [32, 16, 32, 32]          1,168\n├─MaxPool2d: 1-4                         [32, 16, 16, 16]          --\n├─Linear: 1-5                            [32, 64]                  262,208\n├─Linear: 1-6                            [32, 32]                  2,080\n├─Linear: 1-7                            [32, 1]                   33\n==========================================================================================\nTotal params: 265,713\nTrainable params: 265,713\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 76.09\n==========================================================================================\nInput size (MB): 1.57\nForward/backward pass size (MB): 12.61\nParams size (MB): 1.06\nEstimated Total Size (MB): 15.24\n=========================================================================================="},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def train(model, train_loader, optimizer, criterion, device, epoch, log_interval, batch_size):\n    model.train()\n    correct = 0\n    train_loss = 0\n    total_samples = 0\n    for batch_idx, (image, label) in enumerate(train_loader):\n        image = image.to(device)\n        label = label.to(device)\n        optimizer.zero_grad()\n        output = model(image)[:,0]\n        loss = criterion(output, label.float())\n        train_loss += loss.item() * label.size(0)  # loss * batch_size로 누적\n        loss.backward()\n        optimizer.step()\n        \n        # batch별 정답수 누적\n        preds = (output >= 0.5).float()\n        correct += (preds == label).float().sum().item()\n        total_samples += label.size(0)\n\n        if batch_idx % log_interval == 0:\n            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n                epoch, batch_idx * image.size(0),\n                len(train_loader.dataset), 100. * batch_idx / len(train_loader),\n                loss.item()))\n           \n    train_loss /= total_samples\n    train_accuracy = 100. * correct / total_samples\n\n    return train_loss, train_accuracy\n\n\n\ndef evaluate(model, test_loader):\n    global output, label\n    model.eval()\n    test_loss = 0\n    correct = 0\n\n    with torch.no_grad():\n        for image, label in test_loader:\n            image = image.to(DEVICE)\n            label = label.to(DEVICE)\n            output = model(image)\n            test_loss += criterion(output.reshape(-1), label.float()).item() * label.size(0)\n            correct += ((output.reshape(-1) >= 0.5).float() == label).float().sum().item()\n\n\n    test_loss /= len(test_loader.dataset)\n    test_accuracy = 100. * correct / len(test_loader.dataset)\n    return test_loss, test_accuracy\n    \nclass early_stopping:\n  def __init__(self, patience, verbose, delta, path='checkpoint.pt'):\n    self.patience = patience\n    self.verbose = verbose\n    self.delta = delta\n    self.count = 0\n    self.best_score = None\n    self.early_stop = False\n    self.val_loss_min = np.inf\n    self.path = path\n\n\n  def __call__(self, val_loss, model):\n    score = -val_loss\n\n\n    if self.best_score is None:\n      self.best_score = score\n      self.save_checkpoint(val_loss, model)\n    elif score < self.best_score + self.delta:\n      self.count += 1\n      if self.verbose:\n        print(f\"Early Stopping counter: {self.count} out of {self.patience}\")\n\n\n      if self.count >= self.patience:\n        self.early_stop = True  \n    else:\n      self.best_score = score\n      self.save_checkpoint(val_loss, model)\n      self.count =0  \n\n\n  def save_checkpoint(self, val_loss, model):\n    if self.verbose:\n      print(f\"Validation loss decreased ({self.val_loss_min:.6f}) --> {val_loss:.6f}. saving model..\")\n    torch.save(model.state_dict(), self.path)\n    self.val_loss_min = val_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T06:44:31.525920Z","iopub.execute_input":"2025-07-03T06:44:31.526620Z","iopub.status.idle":"2025-07-03T06:44:31.543472Z","shell.execute_reply.started":"2025-07-03T06:44:31.526587Z","shell.execute_reply":"2025-07-03T06:44:31.542499Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import numpy as np\nearly_stopping = early_stopping(patience=2, verbose=True, path='best_model.pt', delta=0)\nloss_hist_train     = [0] * EPOCHS\naccuracy_hist_train = [0] * EPOCHS\nloss_hist_valid     = [0] * EPOCHS\naccuracy_hist_valid = [0] * EPOCHS\nfor epoch in range(1, EPOCHS + 1):\n    loss_, acc_ = train(model, train_loader, optimizer, criterion, DEVICE, epoch, log_interval = 200, batch_size=BATCH_SIZE)\n    loss_hist_train[epoch-1] = loss_\n    accuracy_hist_train[epoch-1] = acc_\n    test_loss, test_accuracy = evaluate(model, test_loader)\n    loss_hist_valid[epoch-1] = test_loss\n    accuracy_hist_valid[epoch-1] = test_accuracy\n    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n        epoch, test_loss, test_accuracy))\n    early_stopping(test_loss, model)\n    if early_stopping.early_stop:\n        print(\"Early stopping\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T06:47:27.821002Z","iopub.execute_input":"2025-07-03T06:47:27.821626Z"}},"outputs":[{"name":"stdout","text":"Train Epoch: 1 [0/141819 (0%)]\tTrain Loss: 0.681397\nTrain Epoch: 1 [6400/141819 (5%)]\tTrain Loss: 0.383078\nTrain Epoch: 1 [12800/141819 (9%)]\tTrain Loss: 0.277953\nTrain Epoch: 1 [19200/141819 (14%)]\tTrain Loss: 0.193259\nTrain Epoch: 1 [25600/141819 (18%)]\tTrain Loss: 0.285941\nTrain Epoch: 1 [32000/141819 (23%)]\tTrain Loss: 0.287659\nTrain Epoch: 1 [38400/141819 (27%)]\tTrain Loss: 0.307902\nTrain Epoch: 1 [44800/141819 (32%)]\tTrain Loss: 0.252554\nTrain Epoch: 1 [51200/141819 (36%)]\tTrain Loss: 0.338288\nTrain Epoch: 1 [57600/141819 (41%)]\tTrain Loss: 0.199007\nTrain Epoch: 1 [64000/141819 (45%)]\tTrain Loss: 0.275837\nTrain Epoch: 1 [70400/141819 (50%)]\tTrain Loss: 0.366775\nTrain Epoch: 1 [76800/141819 (54%)]\tTrain Loss: 0.305120\nTrain Epoch: 1 [83200/141819 (59%)]\tTrain Loss: 0.284206\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}